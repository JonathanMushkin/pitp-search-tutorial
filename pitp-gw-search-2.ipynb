{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "This assignment is the first part of two exercises, in which we will analyze LIGO data to find the gravitational wave transient caused by the coalescence of two neutron stars (GW170817).\n",
    "\n",
    "You are given a true 2048-second segment of Hanford LIGO data, sampled at 4096 Hz (down-sampled from the original 16 kHz data). Along with this PDF, you should have:\n",
    "\n",
    "1. `strain.npy`, readable by NumPy, containing the strain data.\n",
    "2. `gw_search_functions`, containing helpful functions, constants.\n",
    "3. The timestamps corresponding to the strain are not uploaded due to size, and are instead provided in `gw_search_functions`.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, create an use a **template-bank**, attempt to find the famous GW170817 event, and place confidence in the detection, in the form of a false-alarm rate.\n",
    "\n",
    "It is advised to get this code from https://github.com/JonathanMushkin/GW_search_tutorial, and use the pyproject.toml to define an environment.\n",
    "\n",
    "Please contact jonathan.mushkin[at]weizmann.ac.il for any help, question or comment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0  \n",
    "Load data, evaluate ASD and whitening filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, stats\n",
    "import gw_search_functions\n",
    "\n",
    "filename = \"strain.npy\"\n",
    "event_name = \"GW170817\"\n",
    "detector_name = \"H\"\n",
    "fs = 2**12  # Hz\n",
    "\n",
    "strain = np.load(filename)\n",
    "times = np.arange(len(strain)) / fs\n",
    "dt = times[1] - times[0]\n",
    "freqs = np.fft.rfftfreq(len(strain), d=dt)\n",
    "df = freqs[1] - freqs[0]\n",
    "\n",
    "tukey_window = signal.windows.tukey(M=len(strain), alpha=0.1)\n",
    "strain_f = np.fft.rfft(strain * tukey_window)\n",
    "\n",
    "seg_duration = 64\n",
    "overlap_duration = 32\n",
    "nperseg = int(seg_duration * fs)\n",
    "noverlap = int(overlap_duration * fs)\n",
    "welch_dict = {\n",
    "    \"x\": strain,\n",
    "    \"fs\": fs,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"noverlap\": noverlap,\n",
    "    \"average\": \"median\",\n",
    "    \"scaling\": \"density\",\n",
    "}\n",
    "psd_freqs, psd_estimation = signal.welch(**welch_dict)\n",
    "asd_estimation = psd_estimation ** (1 / 2)\n",
    "fmin = 20\n",
    "asd = np.interp(freqs, psd_freqs, asd_estimation)\n",
    "\n",
    "# Create high-pass filter\n",
    "# make it go like sin-squared from 0 to 1 over (fmin, fmin+1Hz) interval\n",
    "highpass_filter = np.zeros(len(freqs))\n",
    "i1, i2 = np.searchsorted(freqs, (fmin, fmin + 1))\n",
    "highpass_filter[i1:i2] = np.sin(np.linspace(0, np.pi / 2, i2 - i1)) ** 2\n",
    "highpass_filter[i2:] = 1.0\n",
    "\n",
    "# whitening filter is 1/asd(f) * high-pass filter\n",
    "whitening_filter_raw = highpass_filter / np.interp(\n",
    "    x=freqs, xp=psd_freqs, fp=asd_estimation\n",
    ")\n",
    "\n",
    "# To avoid ripples in Fourier domain, we apply a windowing in time domain\n",
    "\n",
    "padded_tukey_window = np.fft.fftshift(\n",
    "    np.pad(\n",
    "        signal.windows.tukey(M=nperseg, alpha=0.1),\n",
    "        pad_width=(len(strain) - nperseg) // 2,\n",
    "        constant_values=0,\n",
    "    )\n",
    ")\n",
    "# tranform to time domain, apply the window, and return to frequency domain\n",
    "whitening_filter = (\n",
    "    highpass_filter\n",
    "    * np.fft.rfft(padded_tukey_window * np.fft.irfft(whitening_filter_raw))\n",
    ").real * np.sqrt(2 * dt)\n",
    "\n",
    "wht_strain_f = strain_f * whitening_filter\n",
    "wht_strain_t = np.fft.irfft(wht_strain_f)\n",
    "\n",
    "i1 = np.searchsorted(freqs, fmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " Now you know how to conduct a search with a single template. We now go on to prepare a bank of templates. \n",
    " \n",
    " The game is make the template bank \"dense\" enough so the mismatch between a true signal in the data is never too large, while making it \"sparse\" enough not to be wasteful. For example, if 2 parameters gives the exact same waveform, it is a shame to include both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1.  Theoretical questions:\n",
    "\n",
    "Assume you have an error budget of 5\\% in the $\\text{SNR}^2$. \n",
    "\n",
    "1. You conduct a search with template $h_1$. The data is $d = n + A h_1$, with amplitude $A$. Find the $\\ln\\mathcal{L}={\\rm SNR}^2/2$. It is useful to use the inner-product notation:\n",
    "\\begin{equation}\n",
    "\\langle a \\mid b \\rangle = \\sum_{f = 0}^{f_{\\rm max}} \\frac{a(f)b^{\\ast}(f)}{S_n(f)}\n",
    "\\end{equation} \n",
    "\n",
    "2. Assume data $d = h_1 + n$. You conduct a search with $h_2\\neq h_1$. Relate the $\\text{SNR}^2$ loss to the overlap between the waveforms, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{O}_{ij} = \\frac{\\vert\\langle h_i \\mid h_j \\rangle\\vert}{\\sqrt{\\langle h_i \\mid h_i \\rangle \\langle h_j \\mid h_j \\rangle}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "\n",
    " ## Template bank\n",
    " \n",
    " One option to create a bank is to draw samples of the template parameters, create all templates. Then, evaluate all overlaps between template pairs :\n",
    "\n",
    " \n",
    " time time and phase shifts are done in the search. But the cost of this process is hugh (scales like bank size squared).\n",
    "\n",
    "If instead we find a good basis to describe the banks, we can evaluate the match / mismatch between templates based on their new found coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Phase templates\n",
    "\n",
    "The main idea (explained in https://arxiv.org/abs/1904.01683) is that the $h_+$ waveform can be represented by an amplitude and phase. For a small enough region of parameter space, the amplitudes are the same, and the phases are mostly similar. It is therefore useful to write the phase $\\Psi(f)$ as a common phase evolution $\\bar{\\Psi}(f)$  (mean over templates per frequency) and a deviation from it. We will write the deviations as a linear combination of orthonormal phase functions:\n",
    "\n",
    "$$\n",
    "\\Psi_i(f) = \\bar{\\Psi}(f) + \\sum_\\alpha c_\\alpha \\psi_\\alpha(f)\n",
    "$$\n",
    "\n",
    "We assume the phases are the linear-free (global phase and time standartization)  as discussed in the previous notebook, and is available at\n",
    "`gw_search_functions.phases_to_linear_free_phases`. \n",
    "\n",
    "Orthonormality is defined using the whitened-amplitude weights:\n",
    "\n",
    "$$\n",
    "\\sum_f \\frac{A^2(f)}{S_n(f)} \\psi_i(f)\\psi_j(f) = \\delta_{ij}\n",
    "$$\n",
    "\n",
    "Given two normalized templates $h_i(f)$, $i=1,2$, the match between the templates is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}_{ij} = \\langle h_i| h_j\\rangle = \\sum_f \\frac{A_i(f) A_j(f)}{S_n(f)} e^{i(\\Psi_1(f)-\\Psi_2(f))} \\, \\mathrm{d}f\n",
    "$$\n",
    "\n",
    "To second order in $\\Delta \\Psi = \\Psi_i - \\Psi_j$:\n",
    "\n",
    "$$\n",
    "\\langle h_i | h_j \\rangle \\approx \\sum_f\\frac{A^2(f)}{S_n(f)} \\left(1 + i \\Delta \\Psi(f) - \\frac{1}{2}(\\Delta\\Psi(f))^2 \\right) \\, \\mathrm{d}f\n",
    "$$\n",
    "\n",
    "The imaginary part will not matter for the SNR calculation, so we ignore it (return to the Gaussian likelihood to understand why). The second-order term becomes:\n",
    "\n",
    "$$\n",
    "\\langle h_i | h_j \\rangle \\approx 1 - \\frac{1}{2} \\sum_\\alpha (\\Delta c_\\alpha)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## SVD of phase-basis\n",
    "We can create an orthonormal basis by taking the sample phases and performing SVD on a matrix $X$ of shape $N_{\\rm samples} × N_{\\rm frequencies}$:\n",
    "\n",
    "$$\n",
    "X_{ij} = \\Psi_i(f_j) \\cdot \\frac{A(f_j)}{\\sqrt{S_n(f_j)}}\n",
    "$$\n",
    "\n",
    "To obtain the desired $\\psi_\\alpha(f)$, divide the resulting linear basis vectors by the weights. The eigenvalues from the SVD indicate how many components are needed to represent the waveform set accurately.\n",
    "\n",
    "\n",
    "**Note:** The phase evolution is smooth. You can downsample the frequency grid starting at 20 Hz with steps of $2^{-4}$ Hz to reduce computational cost. Performing SVD on the full-resolution grid may be too demanding for standard hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples = 2**6\n",
    "m1, m2 = gw_search_functions.draw_mass_samples(n_samples)\n",
    "\n",
    "\n",
    "plt.scatter(m1,m2)\n",
    "plt.xlabel(r\"$m_1\\; (M_\\odot)$\")\n",
    "plt.xlabel(r\"$m_2\\; (M_\\odot)$\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = 20\n",
    "freq_jump = 1 / 2**4\n",
    "step_jump = int(freq_jump / df)\n",
    "fslice = slice(np.searchsorted(freqs, (fmin+0.5)), len(freqs), 128)\n",
    "f_sparse = freqs[fslice]  # spareset frequency grid\n",
    "\n",
    "amp = f_sparse ** (-7 / 6)\n",
    "wht_amp = amp * whitening_filter[fslice]\n",
    "wht_amp = wht_amp / np.sqrt(np.sum(wht_amp**2))  # renormalize\n",
    "\n",
    "Psi = gw_search_functions.masses_to_phases(m1, m2, f_sparse)\n",
    "Psi_linear_free = gw_search_functions.phases_to_linear_free_phases(\n",
    "    Psi, f_sparse, wht_amp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "1. Pefrom the SVD, using the proper weights, not including the common phase evolution term. \n",
    "\n",
    "2. Plot the eigen-values of the different components, on a logarithmic y-axis plot. How many components will you take? Why?\n",
    "\n",
    "3. Plot all (or some) of the phases (without the common phase evolution) against frequency. Reconstruct the phases using the smaller number of components. Plot the residuals against frequency. Are you content with the phase differences? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_phase_evolution = 0 # wrong\n",
    "phases_without_common_evolution = Psi_linear_free # wrong\n",
    "svd_phase = phases_without_common_evolution\n",
    "svd_weights = 1 # wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could take up to 1-5 minutes.\n",
    "u, d, v = np.linalg.svd(svd_phase * svd_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(d, \".\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cut the number of components to the approximation level you want\n",
    "ndim = len(d) # wrong\n",
    "u = u[:, :ndim]\n",
    "d = d[:ndim]\n",
    "v = v[:ndim, :]\n",
    "\n",
    "# create a phase vector (without weights) from SVD components\n",
    "# and new set of coordiantes\n",
    "coordinates = u # wrong\n",
    "phase_basis_sparse_freqs = v # wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1, nrows=2)\n",
    "_ = axs[0].plot(f_sparse, \n",
    "         phases_without_common_evolution.T)\n",
    "\n",
    "axs[0].set_ylabel(r\"Deviation from $\\bar{\\Psi}$\")\n",
    "axs[0].set_xlabel(\"Frequency (Hz)\")\n",
    "residual = phases_without_common_evolution - coordinates @ phase_basis_sparse_freqs\n",
    "\n",
    "_ = axs[1].plot(f_sparse, residual.T)\n",
    "axs[1].set_ylabel(\"Residuals\")\n",
    "axs[1].set_xlabel(\"Frequency (Hz)\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Interpolate the phase-basis to full frequency resolution, so the templates can be correlated with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phase_basis = np.array(\n",
    "    [\n",
    "        np.interp(x=freqs, xp=freqs[fslice], fp=phase_base, left=0)\n",
    "        for phase_base in phase_basis_sparse_freqs\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  2\n",
    "\n",
    "Let us check our underlying assumption about the coordinates & overlap relation. \n",
    "\n",
    "Create a template with some coordinates $\\mathbf{c}=\\{c^{\\alpha=1},c^{\\alpha=2}...,\\}$. \n",
    "\n",
    "Creaet more template with increasingly greater distance, a distance of 1.\n",
    "\n",
    "Alternatively, use the already existing phases and their coordinates. \n",
    "\n",
    "**Plot the coordinate distance between the tempaltes to the rest evaluated using the full inner product, and the one assumed from the coordinate distance.**\n",
    "\n",
    "Does the results agree? Do they fully agree? Do you expect them to fully agree? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# 3 Bank Creation with Random Placement\n",
    "\n",
    "Following the option suggested by Barak in the lecture, we will attempt the \"brute force\" random placement approach. \n",
    "\n",
    "It is possible to try other methods. \n",
    "\n",
    "1. For the random placement method, draw $2^{13}$ mass samples. Create the phase for each, and find the coordinates of each. Use the coordinates / vectors found in the SVD from the last section.\n",
    "\n",
    "2. Use to coordinates, select a subset such that the distance between any 2 samples is not smaller than 0.1. It can be done iteratively : Look at a template. Compare it to the accepted templates, and accept / reject it. Repeat for the next tepplate, and so on. You can implement it youself, or use the function `gw_search_functions.select_points_without_clutter`.\n",
    "\n",
    "3. On the same plot, create a scatter plot of the coordinates of the $2^{13}$ samples and of the selected subset.\n",
    "\n",
    "4. On the plot, write down the size of subset. This subset defines the search bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1, m2 = gw_search_functions.draw_mass_samples(2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phases_on_coarse_freqs = gw_search_functions.masses_to_phases(m1, m2, f_sparse)\n",
    "linear_free_phases = gw_search_functions.phases_to_linear_free_phases(\n",
    "    phases_on_coarse_freqs, f_sparse, wht_amp\n",
    ")\n",
    "phases_without_common_evolution = linear_free_phases - common_phase_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates = (\n",
    "    svd_weights**2 * phases_without_common_evolution\n",
    ") @ phase_basis_sparse_freqs.T\n",
    "\n",
    "distance_scale = 1 # wrong\n",
    "\n",
    "bank_coordinates, bank_indices = (\n",
    "    gw_search_functions.select_points_without_clutter(\n",
    "        coordinates, distance_scale\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    *coordinates.T,\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=\"r\",\n",
    "    label=f\"full set ({len(coordinates)} points)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    *bank_coordinates.T,\n",
    "    s=5,\n",
    "    c=\"k\",\n",
    "    label=f\"subset ({len(bank_coordinates)} points)\",\n",
    ")\n",
    "print(bank_coordinates.shape)\n",
    "plt.legend(bbox_to_anchor=(1.01, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Create whitened-amplitudes for the templates. Make sure they are properly normalized for the correlation functions, which isn't necessarily the same as for the inner product used in previous sections.\n",
    "\n",
    "In the previous exercise we defined the correlation function as \n",
    "\n",
    "`correlation = np.fft.irfft( data_wht * template_wht.conj())`\n",
    "\n",
    "You can use the `gw_search_functions.correlate`.\n",
    "\n",
    "You can either implememt you own SNR calculation code, or us `snr2_timeseries` and `complex_overlap_timeseries` in `gw_search_functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amp = np.zeros_like(freqs)\n",
    "amp[i1:] = freqs[i1:] ** (-7 / 6)\n",
    "wht_amp = amp * whitening_filter\n",
    "normalization = 1 # wrong\n",
    "amp /= normalization\n",
    "wht_amp /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 4 The Search\n",
    "\n",
    "A search actually includes quite a lot of by-products. In the following we will try to conduct a search while also not keeping too much un-wanted information. \n",
    "\n",
    "**Before using the entire bank, try a small subset and see that the results make sense. The entire search could take several minutes, depending on hardware.**\n",
    "\n",
    "1. Do for each template in the bank individually (including glitch-removal). \n",
    "\n",
    "2. For each interval of 0.1 seconds, record which template gave the maximal SNR, and what was that SNR.\n",
    "\n",
    "3. Plot the time-series of maximal $\\text{SNR}^2$ in per 0.1 seconds. \n",
    "\n",
    "4. Plot a histogram of the maximal values per 0.1 seconds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is how we found the frequency-split to h_low and h_high\n",
    "\n",
    "frac_snr2 = np.cumsum(np.abs(wht_amp) ** 2)\n",
    "frac_snr2 /= frac_snr2[-1]\n",
    "j = np.searchsorted(frac_snr2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lists = []\n",
    "snr2_lists = []\n",
    "glitch_mask_list = []\n",
    "snr2_lists_raw = []\n",
    "indices_lists_raw = []\n",
    "\n",
    "common_phase_evolution_high_res = np.interp(\n",
    "    x=freqs, xp=f_sparse, fp=common_phase_evolution\n",
    ")\n",
    "\n",
    "f_sampling = 1 / dt\n",
    "t_start = time.time()\n",
    "segment_for_maximization = int(0.1 * fs)\n",
    "glitch_test_threshold = stats.chi2(df=2).isf(0.01)\n",
    "min_snr2_for_glitch_removal = 10\n",
    "\n",
    "for template_index, template_coordinate in tqdm(\n",
    "    enumerate(bank_coordinates),\n",
    "    total=len(bank_coordinates),\n",
    "    desc=\"Conducting a search\",\n",
    "):\n",
    "    phase = common_phase_evolution_high_res + template_coordinate @ phase_basis\n",
    "\n",
    "    # conduct a search without caring about glitchs\n",
    "    wht_h = wht_amp * np.exp(1j * phase)\n",
    "    snr2 = ()\n",
    "\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2, segment_for_maximization\n",
    "    )\n",
    "    indices_lists_raw.append(argmaxs)\n",
    "    snr2_lists_raw.append(maxs)\n",
    "\n",
    "    # conduct the search with caring about glitchsß\n",
    "    wht_h_low, wht_h_high = np.zeros((2, len(freqs)), complex)\n",
    "    wht_h_low[:j] = wht_h[:j]\n",
    "    wht_h_high[j:] = wht_h[j:]\n",
    "    z_low = ()\n",
    "    z_high = ()\n",
    "\n",
    "    glitch_test_statistic = np.abs(z_low - z_high) ** 2\n",
    "    glitch_mask = ()\n",
    "    glitch_mask_list.append(glitch_mask)\n",
    "\n",
    "    snr2_after_glitch_test = snr2 * ~glitch_mask\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2_after_glitch_test, segment_for_maximization\n",
    "    )\n",
    "    indices_lists_raw.append(argmaxs)\n",
    "    snr2_lists.append(maxs)\n",
    "\n",
    "snr2_per_template = np.array(snr2_lists)\n",
    "time_indices_per_template = np.array(indices_lists)\n",
    "snr2_per_template_raw = np.array(snr2_lists_raw)\n",
    "time_indices_per_template_raw = np.array(indices_lists_raw)\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_times = np.linspace(0, times[-1], snr2_per_template.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1, sharex=True, sharey=True)\n",
    "axs[0].plot(binned_times, np.zeros_like(binned_times))\n",
    "axs[0].set_xlabel(\"time (s)\")\n",
    "axs[0].set_ylabel(r\"Bestfit ${\\rm SNR}^2$ without glitch removal\")\n",
    "\n",
    "axs[1].plot(binned_times, np.zeros_like(binned_times))\n",
    "axs[1].set_xlabel(\"time (s)\")\n",
    "axs[1].set_ylabel(r\"Bestfit ${\\rm SNR}^2$ with glitch removal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    np.zeros_like(binned_times),\n",
    "    **hist_kwargs,\n",
    "    alpha=0.5,\n",
    "    label=\"With glitch removal\",\n",
    ")\n",
    "\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    np.zeros_like(binned_times),\n",
    "    label=\"Before glitch removal\",\n",
    "    **hist_kwargs,\n",
    "    ls=\"--\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"${\\rm SNR}^2$\")\n",
    "ax.set_ylabel(\"counts (normalized)\")\n",
    "leg = ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 5.\n",
    "If you detected an event, **report its time, the masses of the template and an estimation or a upper bound of the false-alarm rate for such SNR**. Consider the number of templates you used and the fact that waveforms have typical auto-correlation length of 1 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with https://arxiv.org/pdf/1710.05832 Table 1\n",
    "best_template_index, best_timestamp_index = np.unravel_index(\n",
    "    snr2_per_template.argmax(), snr2_per_template.shape\n",
    ")\n",
    "\n",
    "bestfit_m1 = 1 # wrong\n",
    "bestfit_m2 = 1 # wrong\n",
    "bestfit_mchirp = gw_search_functions.m1m2_to_mchirp(bestfit_m1, bestfit_m2)\n",
    "bestfit_snr2 = snr2_per_template.max()\n",
    "bestfit_time = snr2_per_template.max(axis=0).argmax()\n",
    "\n",
    "print(f\"Maximal SNR^2 found : {bestfit_snr2:.5g} at time {bestfit_time:.4f}\")\n",
    "print(\n",
    "    f\"Template of masses ({bestfit_m1:.3g},{bestfit_m2:.3g}), or chirp-mass {bestfit_mchirp:.5g} (solar masses)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# 5 Post analysis\n",
    "\n",
    "1. What is the time of detection, masses of best-fit template, and chirp-mass of it?\n",
    "\n",
    "2. What is the False-alarm-rate? Consider the number of attemps, based on the number of templates, duration of the data, and auto-correlation length of the templates (roughly 1ms). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 6. Look at the spectrum\n",
    "\n",
    "Create a spectogram (using e.g. `matplotlib.pyplot.specgram`), localized in time and frequency around the event you found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the histogram in 2 steps. So I can calibrate the dynamic range in the second histogram using the fist histogram\n",
    "specgram_kwargs = {\n",
    "    \"x\": np.fft.irfft(strain_f * whitening_filter),\n",
    "    \"NFFT\": int(fs * 0.5),\n",
    "    \"noverlap\": int(fs * 0.25),\n",
    "    \"scale\": \"linear\",\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 25,\n",
    "    \"Fs\": fs,\n",
    "}\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "specgram_kwargs = {}  # fill it with relelvant parameters\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)\n",
    "tmin = 1 # probably wrong\n",
    "tmax = 2 # probably wrong\n",
    "fmin = 20 # probably ok\n",
    "fmax = 1000 # probably ok\n",
    "plt.xlim(tmin, tmax)\n",
    "plt.ylim(fmin, fmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw_detection_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
