{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "This assignment is the first part of two exercises, in which we will analyze LIGO data to find the gravitational wave transient caused by the coalescence of two neutron stars (GW170817).\n",
    "\n",
    "You are given a true 2048-second segment of Hanford LIGO data, sampled at 4096 Hz (down-sampled from the original 16 kHz data). Along with this PDF, you should have:\n",
    "\n",
    "1. `strain.npy`, readable by NumPy, containing the strain data.\n",
    "2. `gw_search_functions`, containing helpful functions, constants.\n",
    "3. The timestamps corresponding to the strain are not uploaded due to size, and are instead provided in `gw_search_functions`.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, create an use a **template-bank**, attempt to find the famous GW170817 event, and place confidence in the detection, in the form of a false-alarm rate.\n",
    "\n",
    "It is advised to get this code from https://github.com/JonathanMushkin/GW_search_tutorial, and use the pyproject.toml to define an environment.\n",
    "\n",
    "Please contact jonathan.mushkin[at]weizmann.ac.il for any help, question or comment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from scipy import signal, stats\n",
    "\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n",
    "\n",
    "TIME0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 \n",
    "Load the time domain data and Fourier transform it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"strain.npy\"\n",
    "filename.exists()\n",
    "event_name = \"GW170817\"\n",
    "detector_name = \"H\"\n",
    "fs = 2**12  # Hz\n",
    "\n",
    "strain = np.load(filename)\n",
    "times = np.arange(len(strain)) / fs\n",
    "dt = times[1] - times[0]\n",
    "freqs = np.fft.rfftfreq(len(strain), d=dt)\n",
    "df = freqs[1] - freqs[0]\n",
    "\n",
    "tukey_window = signal.windows.tukey(M=len(strain), alpha=0.1)\n",
    "strain_f = np.fft.rfft(strain * tukey_window)\n",
    "\n",
    "seg_duration = 64\n",
    "overlap_duration = 32\n",
    "nperseg = int(seg_duration * fs)\n",
    "noverlap = int(overlap_duration * fs)\n",
    "welch_dict = {\n",
    "    \"x\": strain,\n",
    "    \"fs\": fs,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"noverlap\": noverlap,\n",
    "    \"average\": \"median\",\n",
    "    \"scaling\": \"density\",\n",
    "}\n",
    "psd_freqs, psd_estimation = signal.welch(**welch_dict)\n",
    "asd_estimation = psd_estimation ** (1 / 2)\n",
    "fmin = 20\n",
    "asd = np.interp(freqs, psd_freqs, asd_estimation)\n",
    "\n",
    "# Create high-pass filter\n",
    "# make it go like sin-squared from 0 to 1 over (fmin, fmin+1Hz) interval\n",
    "highpass_filter = np.zeros(len(freqs))\n",
    "i1, i2 = np.searchsorted(freqs, (fmin, fmin + 1))\n",
    "highpass_filter[i1:i2] = np.sin(np.linspace(0, np.pi / 2, i2 - i1)) ** 2\n",
    "highpass_filter[i2:] = 1.0\n",
    "\n",
    "# whitening filter is 1/asd(f) * high-pass filter\n",
    "whitening_filter_raw = highpass_filter / np.interp(\n",
    "    x=freqs, xp=psd_freqs, fp=asd_estimation\n",
    ")\n",
    "\n",
    "# To avoid ripples in Fourier domain, we apply a windowing in time domain\n",
    "\n",
    "padded_tukey_window = np.fft.fftshift(\n",
    "    np.pad(\n",
    "        signal.windows.tukey(M=nperseg, alpha=0.1),\n",
    "        pad_width=(len(strain) - nperseg) // 2,\n",
    "        constant_values=0,\n",
    "    )\n",
    ")\n",
    "# tranform to time domain, apply the window, and return to frequency domain\n",
    "whitening_filter = (\n",
    "    highpass_filter\n",
    "    * np.fft.rfft(padded_tukey_window * np.fft.irfft(whitening_filter_raw))\n",
    ").real * np.sqrt(2 * dt)\n",
    "\n",
    "wht_strain_f = strain_f * whitening_filter  # this is wrong\n",
    "wht_strain_t = np.irfft(wht_strain_f)  # this is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2\n",
    "Estimate the ASD and create the whitening filter. **Create a log-log plot of the ASD from 20Hz onward**. **Create a log-log plot of the whitening filter from 20Hz onward. Plot the entire whitened strain data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "psd_freqs, psd_estimation = signal.welch(**welch_dict)\n",
    "fmin = 20\n",
    "asd = np.interp(freqs, psd_freqs, psd_estimation ** (1 / 2))\n",
    "i = np.searchsorted(psd_freqs, fmin)\n",
    "plt.loglog(psd_freqs[i:], psd_estimation[i:])\n",
    "plt.title(\"ASD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create high-pass filter\n",
    "highpass_filter = np.zeros(len(freqs))\n",
    "i1, i2 = np.searchsorted(freqs, (fmin, fmin + 1))\n",
    "highpass_filter[i1:i2] = np.sin(np.linspace(0, np.pi / 2, i2 - i1)) ** 2\n",
    "highpass_filter[i2:] = 1.0\n",
    "\n",
    "whitening_filter_raw = highpass_filter / np.interp(\n",
    "    x=freqs, xp=psd_freqs, fp=psd_estimation ** (1 / 2)\n",
    ")\n",
    "\n",
    "padded_tukey_window = np.pad(\n",
    "    signal.windows.tukey(M=int(64 * fs), alpha=0.1),\n",
    "    pad_width=(len(strain) - int(64 * fs)) // 2,\n",
    "    constant_values=0,\n",
    ")\n",
    "\n",
    "whitening_filter = (\n",
    "    highpass_filter\n",
    "    * np.fft.rfft(\n",
    "        np.fft.fftshift(\n",
    "            padded_tukey_window\n",
    "            * np.fft.fftshift(np.fft.irfft(whitening_filter_raw))\n",
    "        )\n",
    "    ).real\n",
    "    * np.sqrt(2 * dt)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "_ = ax.loglog(freqs[i1:], np.abs(whitening_filter[i1:]))\n",
    "_ = ax.set_ylim(1e18)\n",
    "_ = ax.set_title(\"whitening filter\")\n",
    "_ = ax.set_xlabel(\"freq [Hz]\")\n",
    "_ = ax.set_ylabel(\"whitening filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(times, np.fft.irfft(strain_f * whitening_filter))\n",
    "plt.title(\"Whitened strain\")\n",
    "plt.xlabel(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 3\n",
    "Create a single template for a search, with arbitrarily selected masses of $m_1=1.5$ and $m_2=1.25$ (in solar masses).  **Plot the time-domain template, such that it is localized in the middle of the time-axis. Fix the plot such that the waveform features are visible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gw_search_functions_SOLVED as gw_search_functions\n",
    "from importlib import reload\n",
    "\n",
    "reload(gw_search_functions)\n",
    "m1 = 1.5\n",
    "m2 = 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i1 = np.searchsorted(freqs, fmin)\n",
    "phase = np.zeros_like(freqs)\n",
    "phase[i1:] = gw_search_functions.masses_to_phases(m1, m2, freqs[i1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amp = np.zeros_like(freqs)\n",
    "amp[i1:] = freqs[i1:] ** (-7 / 6)\n",
    "h = amp * np.exp(1j * phase)\n",
    "normalization = np.fft.irfft(np.abs(h * whitening_filter) ** 2)[0] ** (1 / 2)\n",
    "h /= normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(times, np.fft.fftshift(np.fft.irfft(h)))\n",
    "plt.xlim(850, 1030)\n",
    "ax.set_xlabel(\"time [sec]\")\n",
    "ax.set_ylabel(\"h [arb.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4 \n",
    "Generate the complex-overlap time-series. **Plot a histogram with the real and imaginary parts of the complex-overlap, in a segment of data without an obvious glitch. Overlay the theoretical predictions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whitened_strain_t = np.fft.irfft(strain_f * whitening_filter.conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zoom on glitch, to see where not to use the overlap timeseries\n",
    "plt.plot(times, whitened_strain_t)\n",
    "plt.xlim(1250, 1260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_cos = np.fft.irfft(\n",
    "    (strain_f * whitening_filter) * (h * whitening_filter).conj()\n",
    ")\n",
    "z_sin = np.fft.irfft(\n",
    "    (strain_f * whitening_filter) * (h * whitening_filter).conj() * 1j\n",
    ")\n",
    "z = z_cos + 1j * z_sin\n",
    "snr2 = np.abs(z) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# indices without a glitch\n",
    "t_start = 200\n",
    "t_end = 1000\n",
    "tslice = slice(*np.searchsorted(times, (t_start, t_end)))\n",
    "# keywords for the histogram\n",
    "hist_kwargs = {\"bins\": 200, \"density\": True, \"log\": True, \"histtype\": \"step\"}\n",
    "# create 2 histograms\n",
    "counts, edges, patches = plt.hist(z_cos[tslice], **hist_kwargs, label=\"z_cos\")\n",
    "counts, edges, patches = plt.hist(z_sin[tslice], **hist_kwargs, label=\"z_sin\")\n",
    "# overlay normal distribution with mu=0 and sigma=1\n",
    "plt.plot(edges, stats.norm().pdf(edges), label=\"normal distribution\")\n",
    "plt.legend(loc=\"lower center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 5\n",
    "\n",
    "Create the ${\\rm SNR}^2$ time series.\n",
    "\n",
    "\n",
    "To verify your result, use the estimated ASD to draw mock data without a GW\n",
    "transient. Create the same ${\\rm SNR}^2$ time-series on this data. **On the same figure, plot\n",
    "the histograms of the ${\\rm SNR}^2$ of the real data and of the mock data. Overlay the\n",
    "theoretical prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the variance equally between the real and imaginary components of strain_f\n",
    "mock_strain_f = [1, 1j] @ stats.norm(scale=asd / np.sqrt(2)).rvs(\n",
    "    size=(2, len(freqs))\n",
    ")\n",
    "# multiply by a factor that relates to the duraion\n",
    "mock_strain_f *= len(times) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.semilogy(freqs, np.abs(strain_f), alpha=0.5, label=\"strain\")\n",
    "plt.semilogy(freqs, np.abs(mock_strain_f), alpha=0.5, label=\"mock_strain\")\n",
    "plt.ylabel(r\"$|{\\rm strain}(f)|$\")\n",
    "plt.xlabel(\"frequency [Hz]\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the SNR^2 for the mock data\n",
    "mock_snr2 = (\n",
    "    np.abs(\n",
    "        np.fft.irfft(\n",
    "            (mock_strain_f * whitening_filter) * (h * whitening_filter).conj()\n",
    "        )\n",
    "        + 1j\n",
    "        * np.fft.irfft(\n",
    "            (mock_strain_f * whitening_filter)\n",
    "            * (h * whitening_filter).conj()\n",
    "            * 1j\n",
    "        )\n",
    "    )\n",
    "    ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_kwargs = {\n",
    "    \"histtype\": \"step\",\n",
    "    \"density\": True,\n",
    "    \"log\": True,\n",
    "    \"bins\": range(200),\n",
    "}\n",
    "\n",
    "counts, edges, patches = plt.hist(\n",
    "    snr2, **hist_kwargs, label=r\"real data SNR$^2$\"\n",
    ")\n",
    "counts, edges, pathes = plt.hist(\n",
    "    mock_snr2, **hist_kwargs, label=r\"mock data SNR$^2$\"\n",
    ")\n",
    "plt.plot(edges, stats.chi2(df=2).pdf(edges), label=r\"$\\chi^2(2)$ pdf\")\n",
    "# focus on interesting portion of the histogram\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(1 / np.diff(edges).mean() / len(snr2) / 10)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6\n",
    "\n",
    "Create a test-statistic to detect glitches. Use it to remove glitches from the ${\\rm SNR}^2$ timeseries.\n",
    "**Plot the cleaned ${\\rm SNR}^{2}$ timeseries histogram. Overlay the theoretical prediction**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find f_bar = where the cumulative SNR2 is equal half the overall SNR2\n",
    "frac_snr2 = np.cumsum(np.abs(h * whitening_filter) ** 2)\n",
    "frac_snr2 /= frac_snr2[-1]\n",
    "i_fbar = np.searchsorted(frac_snr2, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the the low and high frequencies templates\n",
    "h_low, h_high = np.zeros((2, len(freqs)), dtype=complex)\n",
    "# normalize them so each has norm 1\n",
    "h_low[:i_fbar] = h[:i_fbar] * np.sqrt(2)\n",
    "h_high[i_fbar:] = h[i_fbar:] * np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check their normalization\n",
    "print(\n",
    "    \"<h|h> = \",\n",
    "    np.fft.irfft((h * whitening_filter) * (h * whitening_filter).conj())[0]\n",
    "    ** (1 / 2),\n",
    ")\n",
    "print(\n",
    "    \"<h_low | h_low> = \",\n",
    "    np.fft.irfft(\n",
    "        (h_low * whitening_filter) * (h_low * whitening_filter).conj()\n",
    "    )[0]\n",
    "    ** (1 / 2),\n",
    ")\n",
    "print(\n",
    "    \"<h_high | h_high> = \",\n",
    "    np.fft.irfft(\n",
    "        (h_high * whitening_filter) * (h_high * whitening_filter).conj()\n",
    "    )[0]\n",
    "    ** (1 / 2),\n",
    ")\n",
    "print(\n",
    "    \"<h_low| h_high> = \",\n",
    "    np.fft.irfft(\n",
    "        (h_high * whitening_filter) * (h_low * whitening_filter).conj()\n",
    "    )[0]\n",
    "    ** (1 / 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_low, z_high = [\n",
    "    (\n",
    "        np.fft.irfft(\n",
    "            strain_f * whitening_filter * (x * whitening_filter).conj()\n",
    "        )\n",
    "        + 1j\n",
    "        * np.fft.irfft(\n",
    "            strain_f * whitening_filter * (x * whitening_filter).conj() * 1j\n",
    "        )\n",
    "    )\n",
    "    for x in [h_low, h_high]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create 2 scatter plots of z_cos-z_sin (real vs imaginary) around and not around a glitch.\n",
    "# NOT ASKED FOR IN THE EXAM\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True)\n",
    "tslice = slice(*np.searchsorted(times, (100, 101)))\n",
    "axs[0].scatter(\n",
    "    (z_low - z_high)[tslice].real,\n",
    "    (z_low - z_high)[tslice].imag,\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    ")\n",
    "axs[0].set_title(\"not around glitch\")\n",
    "tslice = slice(*np.searchsorted(times, (1258, 1259)))\n",
    "axs[1].scatter(\n",
    "    (z_low - z_high)[tslice].real,\n",
    "    (z_low - z_high)[tslice].imag,\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    ")\n",
    "axs[1].set_title(\"around glitch\")\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(r\"$z_\\cos - z_\\sin$ (real)\")\n",
    "    ax.set_ylabel(r\"$z_\\cos - z_\\sin$ (imaginary)\")\n",
    "    ax.text(0.05, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glitch_test_statistic = 0.5 * np.abs(z_low - z_high) ** 2\n",
    "glitch_test_threshold = stats.chi2(df=2).isf(\n",
    "    0.01\n",
    ")  # throw away one in a 100 good signals\n",
    "glitch_mask = (glitch_test_statistic > glitch_test_threshold) * (snr2 > 5**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "hist_kwargs = {\n",
    "    \"histtype\": \"step\",\n",
    "    \"density\": True,\n",
    "    \"log\": True,\n",
    "    \"bins\": range(200),\n",
    "}\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2, **hist_kwargs, label=r\"SNR$^2$ before glitch-vetoing\"\n",
    ")\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2[~glitch_mask], **hist_kwargs, label=r\"SNR$^2$ after glitch vetoing\"\n",
    ")\n",
    "\n",
    "\n",
    "ax.plot(edges, stats.chi2(df=2).pdf(edges), label=r\"$\\chi^2(2)$\")\n",
    "y_lower_limit = 0.5 / (np.diff(edges).mean() * len(snr2))\n",
    "ax.set_xlim(right=100)\n",
    "ax.set_ylim(y_lower_limit)\n",
    "leg = ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# 7\n",
    "\n",
    " Now you know how to conduct a search with a single template. We now go on to prepare a bank of templates. The first step is to find a good linear basis to work with. Draw $\\sim2^8$ mass samples (see given functions). Create the waveform for each and perform SVD to find their phase basis-vectors. Choose the number of basis-vectors you want to use. **Plot the basis-vectors against frequency**. Make sure to read the guidance before performing the SVD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1, m2 = gw_search_functions.draw_mass_samples(2**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take sparser frequency grid\n",
    "fslice = slice(np.searchsorted(freqs, (fmin)), len(freqs), 128)\n",
    "fs = freqs[fslice]\n",
    "phases = np.array(\n",
    "    [\n",
    "        gw_search_functions.masses_to_phases(mm1, mm2, fs)\n",
    "        for mm1, mm2 in zip(m1, m2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wht_amp = (amp * whitening_filter)[fslice]\n",
    "wht_amp = wht_amp / np.sqrt(np.sum(wht_amp**2))  # renormalize\n",
    "\n",
    "linear_free_phases = gw_search_functions.phases_to_linear_free_phases(\n",
    "    phases, freqs[fslice], weights=wht_amp\n",
    ")\n",
    "common_phase_evolution = linear_free_phases.mean(axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freqs[fslice], linear_free_phases.T)\n",
    "_ = ax.plot(freqs[fslice], common_phase_evolution, ls=\"--\", c=\"k\")\n",
    "ax.text(0.75, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phases_without_common_evolution = linear_free_phases - common_phase_evolution\n",
    "svd_phase = phases_without_common_evolution\n",
    "svd_weights = wht_amp\n",
    "print(svd_weights.shape, svd_phase.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# could take up to 1-5 minutes.\n",
    "u, d, v = np.linalg.svd(svd_phase * svd_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(d[:20], \".\")\n",
    "ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)\n",
    "ax.text(\n",
    "    0.5,\n",
    "    0.75,\n",
    "    r\"This is why we take 2 components\",\n",
    "    color=\"red\",\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check that eignen vectors has zero weighted mean. Meaning, they are orthogonal to a constant function.\n",
    "(v[0] * svd_weights).mean(), (v[1] * svd_weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = Path(\"local_outputs\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "np.savez(\n",
    "    output_dir / \"GW170817_H_svd\", u=u, d=d, v=v, freqs_sliced=fs, freqs=freqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u, d, v = [\n",
    "    np.load(output_dir / \"GW170817_H_svd.npz\").get(k) for k in (\"u\", \"d\", \"v\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick 2 coordinates\n",
    "ndim = 2\n",
    "u = u[:, :ndim]\n",
    "d = d[:ndim]\n",
    "v = v[:ndim, :]\n",
    "\n",
    "# create a phase vector (without weights) from SVD components\n",
    "# and new set of coordiantes\n",
    "coordinates = u * d\n",
    "\n",
    "phase_basis_coarse_freqs = np.zeros_like(v)\n",
    "mask = svd_weights != 0\n",
    "phase_basis_coarse_freqs[:, mask] = v[:, mask] / svd_weights[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tests\n",
    "print(\"NOT REQUIRED\")\n",
    "print(\"SVD weights norm:\", np.sum(svd_weights**2))\n",
    "print(\n",
    "    \"First basis norm:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0] ** 2),\n",
    ")\n",
    "print(\n",
    "    \"Const. component in first basis:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[1]),\n",
    ")\n",
    "print(\n",
    "    \"Second basis norm:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0] ** 2),\n",
    ")\n",
    "print(\n",
    "    \"Const. component in second basis:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(phase_basis_coarse_freqs[0], label=\"1st component\")\n",
    "ax.plot(phase_basis_coarse_freqs[1], label=\"2nd component\")\n",
    "ax.grid()\n",
    "leg = ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create full-frequency resolution phase basis\n",
    "phase_basis = np.array(\n",
    "    [\n",
    "        np.interp(x=freqs, xp=freqs[fslice], fp=phase_base, left=0)\n",
    "        for phase_base in phase_basis_coarse_freqs\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  8\n",
    "\n",
    "Calculate the inner product between waveforms at different coordinate-distance $\\sqrt{\\sum_\\alpha |\\Delta c_\\alpha |^2)}$.\n",
    "**Plot their overlap against their distance, and the theoretical prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances = 10 ** np.linspace(-2, 0.5, num=30)\n",
    "amp = np.zeros_like(freqs[fslice])\n",
    "cond = whitening_filter[fslice] != 0\n",
    "amp[cond] = freqs[fslice][cond] ** (-7 / 6)\n",
    "amp_wht = amp * whitening_filter[fslice]\n",
    "amp_wht /= np.sqrt(np.sum((amp_wht) ** 2))\n",
    "\n",
    "matches = np.zeros_like(distances)\n",
    "for i, distance in enumerate(distances):\n",
    "    dist_per_coordinate = distance / np.sqrt(2)\n",
    "    coordinate = np.array([dist_per_coordinate, dist_per_coordinate])\n",
    "    phase = coordinate @ phase_basis_coarse_freqs + common_phase_evolution\n",
    "    matches[i] = np.sum(\n",
    "        amp_wht\n",
    "        * np.exp(-1j * phase)\n",
    "        * amp_wht\n",
    "        * np.exp(+1j * common_phase_evolution)\n",
    "    ).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(distances**2, 1 - matches, label=\"$1 - $ match\")\n",
    "ax.loglog(\n",
    "    distances**2,\n",
    "    distances**2 / 2,\n",
    "    \".\",\n",
    "    label=r\"$1-\\frac{1}{2}\\sum_\\alpha|\\Delta c_\\alpha |^2$\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"distance = $\\sum_\\alpha |\\Delta c_\\alpha |^2$\")\n",
    "ax.set_ylabel(\"mismatch = $1 - $match\")\n",
    "leg = ax.legend(loc=\"upper left\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "# 9 \n",
    "\n",
    "Draw $2^{13}$ mass samples. Create the phase for each, and find the coordinates of each.\n",
    "Select a subset such that the distance between any 2 samples is not smaller than 0.1. \n",
    "\n",
    "On the same plot, create a scatter plot of the 213 samples and of the selected subset.\n",
    "On the plot, write down the size of subset. This subset defines the search bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fslice = slice(np.searchsorted(freqs, (fmin)), len(freqs), 128)\n",
    "freqs_low_res = freqs[fslice]\n",
    "\n",
    "m1, m2 = gw_search_functions.draw_mass_samples(2**13)\n",
    "redshift = 0.01\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(m1, m2, marker=\".\", s=1)\n",
    "ax.scatter(1.46 * (1 + redshift), 1.27 * (1 + redshift), s=100, marker=\"x\")\n",
    "ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phases_on_coarse_freqs = np.array(\n",
    "    [\n",
    "        gw_search_functions.masses_to_phases(mm1, mm2, freqs_low_res)\n",
    "        for mm1, mm2 in zip(m1, m2)\n",
    "    ]\n",
    ")\n",
    "linear_free_phases = gw_search_functions.phases_to_linear_free_phases(\n",
    "    phases_on_coarse_freqs, freqs_low_res, amp_wht\n",
    ")\n",
    "\n",
    "phases_without_common_evolution = linear_free_phases - common_phase_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1)\n",
    "\n",
    "axs[0].plot(freqs_low_res, linear_free_phases[:64].T)\n",
    "axs[0].plot(freqs_low_res, common_phase_evolution, ls=\"--\", c=\"k\")\n",
    "axs[1].plot(freqs_low_res, phases_without_common_evolution[:64].T)\n",
    "for ax in axs:\n",
    "    ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates = (\n",
    "    svd_weights**2 * phases_without_common_evolution\n",
    ") @ phase_basis_coarse_freqs.T\n",
    "\n",
    "bank_coordinates, bank_indices = (\n",
    "    gw_search_functions.select_points_without_clutter(\n",
    "        coordinates, np.sqrt(0.1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    *coordinates.T,\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=\"r\",\n",
    "    label=f\"full set ({len(coordinates)} points)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    *bank_coordinates.T,\n",
    "    s=5,\n",
    "    c=\"k\",\n",
    "    label=f\"subset ({len(bank_coordinates)} points)\",\n",
    ")\n",
    "print(bank_coordinates.shape)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amp = np.zeros_like(freqs)\n",
    "amp[i1:] = freqs[i1:] ** (-7 / 6)\n",
    "normalization = gw_search_functions.correlate(amp, amp, w=whitening_filter)[\n",
    "    0\n",
    "] ** (1 / 2)\n",
    "amp /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# 10\n",
    "\n",
    "Repeat the search (sections 4-6, without repeating their plots) for each template in the bank individually (including glitch-removal). For each interval of 0.1 seconds, record which template gave the maximal SNR, and what was that SNR. **Plot the time-series of maximal SNR$^2$ in per 0.1 seconds. Plot a histogram of the maximal values per 0.1 seconds**. *Before using the entire bank, try a small subset and see that the results make sense. The entire search could take several minutes, depending on hardware*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lists = []\n",
    "snr2_lists = []\n",
    "min_snr2_to_save = stats.chi2(df=2).isf(1 / (times[-1] / 0.1))\n",
    "glitch_test_threshold = stats.chi2(df=2).isf(0.01)\n",
    "snr2_lists_raw = []\n",
    "indices_lists_raw = []\n",
    "glitch_mask_list = []\n",
    "common_phase_evolution_high_res = np.interp(\n",
    "    x=freqs, xp=freqs_low_res, fp=common_phase_evolution\n",
    ")\n",
    "fs = 1 / dt\n",
    "t_start = time.time()\n",
    "\n",
    "for template_index, template_coordinate in tqdm(\n",
    "    enumerate(bank_coordinates), total=len(bank_coordinates), desc=\"template\"\n",
    "):\n",
    "    phase = common_phase_evolution_high_res + template_coordinate @ phase_basis\n",
    "    h = amp * np.exp(1j * phase)\n",
    "\n",
    "    snr2 = gw_search_functions.snr2_timeseries(\n",
    "        h * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "    h_low, h_high = np.zeros((2, len(freqs)), complex)\n",
    "    h_low[:i_fbar] = h[:i_fbar]\n",
    "    h_high[i_fbar:] = h[i_fbar:]\n",
    "    z_low = gw_search_functions.complex_overlap_timeseries(\n",
    "        h_low * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "    z_high = gw_search_functions.complex_overlap_timeseries(\n",
    "        h_high * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "\n",
    "    glitch_test_statistic = np.abs(z_low - z_high) ** 2\n",
    "\n",
    "    glitch_mask = (glitch_test_statistic > glitch_test_threshold) * (snr2 > 10)\n",
    "    glitch_mask_list.append(glitch_mask)\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2 * ~glitch_mask, int(0.1 * fs)\n",
    "    )\n",
    "    indices_lists.append(argmaxs)\n",
    "    snr2_lists.append(maxs)\n",
    "\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2, int(0.1 * fs)\n",
    "    )\n",
    "    indices_lists_raw.append(argmaxs)\n",
    "    snr2_lists_raw.append(maxs)\n",
    "\n",
    "snr2_per_template = np.array(snr2_lists)\n",
    "time_indices_per_template = np.array(indices_lists)\n",
    "snr2_per_template_with_glitches = np.array(snr2_lists_raw)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bins = np.linspace(0, times[-1], snr2_per_template.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(time_bins, snr2_per_template.max(axis=0))\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_ylabel(r\"Bestfit ${\\rm SNR}^2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2_per_template.max(axis=0),\n",
    "    **hist_kwargs,\n",
    "    alpha=0.5,\n",
    "    label=\"With glitch removal\",\n",
    ")\n",
    "\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2_per_template_with_glitches.max(axis=0).flatten(),\n",
    "    label=\"Before glitch removal\",\n",
    "    **hist_kwargs,\n",
    "    ls=\"--\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"${\\rm SNR}^2$\")\n",
    "ax.set_ylabel(\"counts (normalized)\")\n",
    "leg = ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## 11.\n",
    "If you detected an event, **report its time, the masses of the template and an estimation or a upper bound of the false-alarm rate for such SNR**. Consider the number of templates you used and the fact that waveforms have typical auto-correlation length of 1 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with https://arxiv.org/pdf/1710.05832 Table 1\n",
    "best_template_index, best_timestamp_index = np.unravel_index(\n",
    "    snr2_per_template.argmax(), snr2_per_template.shape\n",
    ")\n",
    "bestfit_m1 = m1[best_template_index]\n",
    "bestfit_m2 = m2[best_template_index]\n",
    "bestfit_mchirp = gw_search_functions.m1m2_to_mchirp(bestfit_m1, bestfit_m2)\n",
    "bestfit_snr2 = snr2_per_template.max()\n",
    "time_bins = np.linspace(0, times[-1], snr2_per_template.shape[1])\n",
    "bestfit_time = snr2_per_template.max(axis=0).argmax()\n",
    "\n",
    "print(\"NOT REQUIRED\")\n",
    "print(f\"Maximal SNR^2 found : {bestfit_snr2:.5g} at time {bestfit_time:.3g}\")\n",
    "print(\n",
    "    f\"Template of masses ({bestfit_m1:.3g},{bestfit_m2:.3g}), or chirp-mass {bestfit_mchirp:.5g} (solar masses)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "\n",
    "p = Probability that non of N = N_templates * N_times individual experiments will reach value x or higher :\n",
    "$$p = 1 - (SF(x))^N$$\n",
    "\n",
    "Probability that at least one of $N$ experiments will reach value x:\n",
    "$1 - p = (SF(x))^N $\n",
    "\n",
    "At this high SNR^2, the FAR is almost exactly zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_templates = bank_coordinates.shape[0]\n",
    "N_trials = N_templates * len(times)\n",
    "\n",
    "stats.chi2(df=2).sf(snr2_per_template.max()) ** N_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.log(1 - stats.chi2(df).sf(snr2_per_template.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {
    "tags": []
   },
   "source": [
    "use binomial to imporve the calculation\n",
    "\n",
    "$$ FAR = 1 - CDF(x)^N = 1 - (1-SF(x))^N \\approx 1 - 1 + N\\cdot SF(x) = N\\cdot SF(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snr2_per_template.size * stats.chi2(df=2).sf(snr2_per_template.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "**Create a spectogram (using e.g. `matplotlib.pyplot.specgram`), localized in time and frequency around the event you found**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the histogram in 2 steps. So I can calibrate the dynamic range in the second histogram using the fist histogram\n",
    "specgram_kwargs = {\n",
    "    \"x\": np.fft.irfft(strain_f * whitening_filter),\n",
    "    \"NFFT\": int(fs * 0.5),\n",
    "    \"noverlap\": int(fs * 0.25),\n",
    "    \"scale\": \"linear\",\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 25,\n",
    "    \"Fs\": fs,\n",
    "}\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "specgram_kwargs = {\n",
    "    \"x\": np.fft.irfft(strain_f * whitening_filter),\n",
    "    \"NFFT\": int(fs * 0.5),\n",
    "    \"noverlap\": int(fs * 0.25),\n",
    "    \"scale\": \"linear\",\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": o[0][(o[1] > 20) * (o[1] < 1000)].std() * 5,\n",
    "    \"Fs\": fs,\n",
    "}\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)\n",
    "tmin = 0.1 * 10259 - 6\n",
    "tmax = 0.1 * 10259 + 1\n",
    "fmin = 20\n",
    "fmax = 1000\n",
    "plt.xlim(tmin, tmax)\n",
    "plt.ylim(20, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME1 = time.time()\n",
    "\n",
    "print(f\"Time passed: {TIME1 - TIME0:.3g} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw_detection_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
