{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "This assignment is the first part of two exercises, in which we will analyze LIGO data to find the gravitational wave transient caused by the coalescence of two neutron stars (GW170817).\n",
    "\n",
    "You are given a true 2048-second segment of Hanford LIGO data, sampled at 4096 Hz (down-sampled from the original 16 kHz data). Along with this PDF, you should have:\n",
    "\n",
    "1. `strain.npy`, readable by NumPy, containing the strain data.\n",
    "2. `gw_search_functions`, containing helpful functions, constants.\n",
    "3. The timestamps corresponding to the strain are not uploaded due to size, and are instead provided in `gw_search_functions`.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, create an use a **template-bank**, attempt to find the famous GW170817 event, and place confidence in the detection, in the form of a false-alarm rate.\n",
    "\n",
    "It is advised to get this code from https://github.com/JonathanMushkin/GW_search_tutorial, and use the pyproject.toml to define an environment.\n",
    "\n",
    "Please contact jonathan.mushkin[at]weizmann.ac.il for any help, question or comment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, stats\n",
    "import gw_search_functions\n",
    "plt.rcParams[\"axes.labelsize\"] = 14\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "plt.rcParams[\"axes.titlesize\"] = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0  \n",
    "Load data, evaluate ASD and whitening filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"strain.npy\"\n",
    "event_name = \"GW170817\"\n",
    "detector_name = \"H\"\n",
    "fs = 2**12  # Hz\n",
    "\n",
    "strain = np.load(filename)\n",
    "times = np.arange(len(strain)) / fs\n",
    "dt = times[1] - times[0]\n",
    "freqs = np.fft.rfftfreq(len(strain), d=dt)\n",
    "df = freqs[1] - freqs[0]\n",
    "\n",
    "tukey_window = signal.windows.tukey(M=len(strain), alpha=0.1)\n",
    "strain_f = np.fft.rfft(strain * tukey_window)\n",
    "\n",
    "seg_duration = 64\n",
    "overlap_duration = 32\n",
    "nperseg = int(seg_duration * fs)\n",
    "noverlap = int(overlap_duration * fs)\n",
    "welch_dict = {\n",
    "    \"x\": strain,\n",
    "    \"fs\": fs,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"noverlap\": noverlap,\n",
    "    \"average\": \"median\",\n",
    "    \"scaling\": \"density\",\n",
    "}\n",
    "psd_freqs, psd_estimation = signal.welch(**welch_dict)\n",
    "asd_estimation = psd_estimation ** (1 / 2)\n",
    "fmin = 20\n",
    "asd = np.interp(freqs, psd_freqs, asd_estimation)\n",
    "\n",
    "# Create high-pass filter\n",
    "# make it go like sin-squared from 0 to 1 over (fmin, fmin+1Hz) interval\n",
    "highpass_filter = np.zeros(len(freqs))\n",
    "i1, i2 = np.searchsorted(freqs, (fmin, fmin + 1))\n",
    "highpass_filter[i1:i2] = np.sin(np.linspace(0, np.pi / 2, i2 - i1)) ** 2\n",
    "highpass_filter[i2:] = 1.0\n",
    "\n",
    "# whitening filter is 1/asd(f) * high-pass filter\n",
    "whitening_filter_raw = highpass_filter / np.interp(\n",
    "    x=freqs, xp=psd_freqs, fp=asd_estimation\n",
    ")\n",
    "\n",
    "# To avoid ripples in Fourier domain, we apply a windowing in time domain\n",
    "\n",
    "padded_tukey_window = np.fft.fftshift(\n",
    "    np.pad(\n",
    "        signal.windows.tukey(M=nperseg, alpha=0.1),\n",
    "        pad_width=(len(strain) - nperseg) // 2,\n",
    "        constant_values=0,\n",
    "    )\n",
    ")\n",
    "# tranform to time domain, apply the window, and return to frequency domain\n",
    "whitening_filter = (\n",
    "    highpass_filter\n",
    "    * np.fft.rfft(padded_tukey_window * np.fft.irfft(whitening_filter_raw))\n",
    ").real * np.sqrt(2 * dt)\n",
    "\n",
    "wht_strain_f = strain_f * whitening_filter\n",
    "wht_strain_t = np.fft.irfft(wht_strain_f)\n",
    "\n",
    "i1 = np.searchsorted(freqs, fmin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    " Now you know how to conduct a search with a single template. We now go on to prepare a bank of templates. \n",
    " \n",
    " The game is make the template bank \"dense\" enough so the mismatch between a true signal in the data is never too large, while making it \"sparse\" enough not to be wasteful. For example, if 2 parameters gives the exact same waveform, it is a shame to include both.\n",
    "\n",
    " One option to create a bank is to draw samples of the template parameters, create all templates. Then, evaluate all overlaps between template pairs :\n",
    "\n",
    " \\begin{equation}\n",
    "\\mathcal{O}_{ij} = \\frac{\\vert\\langle h_i \\mid h_j \\rangle\\vert}{\\sqrt{\\langle h_i \\mid h_i \\rangle \\langle h_j \\mid h_j \\rangle}}\n",
    " \\end{equation}\n",
    " time time and phase shifts are done in the search. But the cost of this process is hugh (scales like bank size squared).\n",
    "\n",
    "If instead we find a good basis to describe the banks, we can evaluate the match / mismatch between templates based on their new found coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "The main idea is that the $h_+$ waveform can be represented by an amplitude and phase. For a small enough region of parameter space, the amplitudes are the same and the phases are a combination of a common phase evolution and a deviation that can be written as a linear combination of orthonormal phase functions:\n",
    "\n",
    "$$\n",
    "\\Psi_i(f) = \\bar{\\Psi}(f) + \\sum_\\alpha c_\\alpha \\psi_\\alpha(f)\n",
    "$$\n",
    "\n",
    "We assume the phases are the linear-free (global phase and time standartization)  as discussed in the previous notebook, and is available at\n",
    "`gw_search_functions.phases_to_linear_free_phases`. \n",
    "\n",
    "Here, the common evolution $\\bar{\\Psi}(f)$ is the mean over samples per frequency. Orthonormality is defined using the whitened-amplitude weights:\n",
    "\n",
    "$$\n",
    "\\sum_f \\frac{A^2(f)}{S_n(f)} \\psi_i(f)\\psi_j(f) = \\delta_{ij}\n",
    "$$\n",
    "\n",
    "Given two normalized templates $h_i(f)$, $i=1,2$, the match between the templates is:\n",
    "\n",
    "$$\n",
    "\\langle h_i| h_j\\rangle = \\sum_f \\frac{A_i(f) A_j(f)}{S_n(f)} e^{i(\\Psi_1(f)-\\Psi_2(f))} \\, \\mathrm{d}f\n",
    "$$\n",
    "\n",
    "To second order in $\\Delta \\Psi = \\Psi_i - \\Psi_j$:\n",
    "\n",
    "$$\n",
    "\\langle h_i | h_j \\rangle \\approx \\sum_f\\frac{A^2(f)}{S_n(f)} \\left(1 + i \\Delta \\Psi(f) - \\frac{1}{2}(\\Delta\\Psi(f))^2 \\right) \\, \\mathrm{d}f\n",
    "$$\n",
    "\n",
    "The imaginary part will not matter for the SNR calculation, so we ignore it (return to the Gaussian likelihood to understand why). The second-order term becomes:\n",
    "\n",
    "$$\n",
    "\\langle h_i | h_j \\rangle \\approx 1 - \\frac{1}{2} \\sum_\\alpha (\\Delta c_\\alpha)^2\n",
    "$$\n",
    "\n",
    "We can create an orthonormal basis by taking the sample phases and performing SVD on a matrix $X$ of shape $N_{\\rm samples} Ã— N_{\\rm frequencies}$:\n",
    "\n",
    "$$\n",
    "X_{ij} = \\Psi_i(f_j) \\cdot \\frac{A(f_j)}{\\sqrt{S_n(f_j)}}\n",
    "$$\n",
    "\n",
    "To obtain the desired $\\psi_\\alpha(f)$, divide the resulting linear basis vectors by the weights. The eigenvalues from the SVD indicate how many components are needed to represent the waveform set accurately.\n",
    "\n",
    "**Note:** The phase evolution is smooth. You can downsample the frequency grid starting at 20 Hz with steps of $2^{-4}$ Hz to reduce computational cost. Performing SVD on the full-resolution grid may be too demanding for standard hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m1, m2 = gw_search_functions.draw_mass_samples(2**8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# take sparser frequency grid\n",
    "fslice = slice(np.searchsorted(freqs, (fmin)), len(freqs), 128)\n",
    "fs = freqs[fslice]\n",
    "phases = np.array(\n",
    "    [\n",
    "        gw_search_functions.masses_to_phases(mm1, mm2, fs)\n",
    "        for mm1, mm2 in zip(m1, m2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wht_amp = (amp * whitening_filter)[fslice]\n",
    "wht_amp = wht_amp / np.sqrt(np.sum(wht_amp**2))  # renormalize\n",
    "\n",
    "linear_free_phases = gw_search_functions.phases_to_linear_free_phases(\n",
    "    phases, freqs[fslice], weights=wht_amp\n",
    ")\n",
    "common_phase_evolution = linear_free_phases.mean(axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(freqs[fslice], linear_free_phases.T)\n",
    "_ = ax.plot(freqs[fslice], common_phase_evolution, ls=\"--\", c=\"k\")\n",
    "ax.text(0.75, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phases_without_common_evolution = linear_free_phases - common_phase_evolution\n",
    "svd_phase = phases_without_common_evolution\n",
    "svd_weights = wht_amp\n",
    "print(svd_weights.shape, svd_phase.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# could take up to 1-5 minutes.\n",
    "u, d, v = np.linalg.svd(svd_phase * svd_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(d[:20], \".\")\n",
    "ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)\n",
    "ax.text(\n",
    "    0.5,\n",
    "    0.75,\n",
    "    r\"This is why we take 2 components\",\n",
    "    color=\"red\",\n",
    "    transform=ax.transAxes,\n",
    ")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check that eignen vectors has zero weighted mean. Meaning, they are orthogonal to a constant function.\n",
    "(v[0] * svd_weights).mean(), (v[1] * svd_weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = Path(\"local_outputs\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "np.savez(\n",
    "    output_dir / \"GW170817_H_svd\", u=u, d=d, v=v, freqs_sliced=fs, freqs=freqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "u, d, v = [\n",
    "    np.load(output_dir / \"GW170817_H_svd.npz\").get(k) for k in (\"u\", \"d\", \"v\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pick 2 coordinates\n",
    "ndim = 2\n",
    "u = u[:, :ndim]\n",
    "d = d[:ndim]\n",
    "v = v[:ndim, :]\n",
    "\n",
    "# create a phase vector (without weights) from SVD components\n",
    "# and new set of coordiantes\n",
    "coordinates = u * d\n",
    "\n",
    "phase_basis_coarse_freqs = np.zeros_like(v)\n",
    "mask = svd_weights != 0\n",
    "phase_basis_coarse_freqs[:, mask] = v[:, mask] / svd_weights[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalization tests\n",
    "print(\"NOT REQUIRED\")\n",
    "print(\"SVD weights norm:\", np.sum(svd_weights**2))\n",
    "print(\n",
    "    \"First basis norm:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0] ** 2),\n",
    ")\n",
    "print(\n",
    "    \"Const. component in first basis:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[1]),\n",
    ")\n",
    "print(\n",
    "    \"Second basis norm:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0] ** 2),\n",
    ")\n",
    "print(\n",
    "    \"Const. component in second basis:\",\n",
    "    np.sum(svd_weights**2 * phase_basis_coarse_freqs[0]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(phase_basis_coarse_freqs[0], label=\"1st component\")\n",
    "ax.plot(phase_basis_coarse_freqs[1], label=\"2nd component\")\n",
    "ax.grid()\n",
    "leg = ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create full-frequency resolution phase basis\n",
    "phase_basis = np.array(\n",
    "    [\n",
    "        np.interp(x=freqs, xp=freqs[fslice], fp=phase_base, left=0)\n",
    "        for phase_base in phase_basis_coarse_freqs\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  8\n",
    "\n",
    "Calculate the inner product between waveforms at different coordinate-distance $\\sqrt{\\sum_\\alpha |\\Delta c_\\alpha |^2)}$.\n",
    "**Plot their overlap against their distance, and the theoretical prediction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distances = 10 ** np.linspace(-2, 0.5, num=30)\n",
    "amp = np.zeros_like(freqs[fslice])\n",
    "cond = whitening_filter[fslice] != 0\n",
    "amp[cond] = freqs[fslice][cond] ** (-7 / 6)\n",
    "amp_wht = amp * whitening_filter[fslice]\n",
    "amp_wht /= np.sqrt(np.sum((amp_wht) ** 2))\n",
    "\n",
    "matches = np.zeros_like(distances)\n",
    "for i, distance in enumerate(distances):\n",
    "    dist_per_coordinate = distance / np.sqrt(2)\n",
    "    coordinate = np.array([dist_per_coordinate, dist_per_coordinate])\n",
    "    phase = coordinate @ phase_basis_coarse_freqs + common_phase_evolution\n",
    "    matches[i] = np.sum(\n",
    "        amp_wht\n",
    "        * np.exp(-1j * phase)\n",
    "        * amp_wht\n",
    "        * np.exp(+1j * common_phase_evolution)\n",
    "    ).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(distances**2, 1 - matches, label=\"$1 - $ match\")\n",
    "ax.loglog(\n",
    "    distances**2,\n",
    "    distances**2 / 2,\n",
    "    \".\",\n",
    "    label=r\"$1-\\frac{1}{2}\\sum_\\alpha|\\Delta c_\\alpha |^2$\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"distance = $\\sum_\\alpha |\\Delta c_\\alpha |^2$\")\n",
    "ax.set_ylabel(\"mismatch = $1 - $match\")\n",
    "leg = ax.legend(loc=\"upper left\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# 9 \n",
    "\n",
    "Draw $2^{13}$ mass samples. Create the phase for each, and find the coordinates of each.\n",
    "Select a subset such that the distance between any 2 samples is not smaller than 0.1. \n",
    "\n",
    "On the same plot, create a scatter plot of the 213 samples and of the selected subset.\n",
    "On the plot, write down the size of subset. This subset defines the search bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fslice = slice(np.searchsorted(freqs, (fmin)), len(freqs), 128)\n",
    "freqs_low_res = freqs[fslice]\n",
    "\n",
    "m1, m2 = gw_search_functions.draw_mass_samples(2**13)\n",
    "redshift = 0.01\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(m1, m2, marker=\".\", s=1)\n",
    "ax.scatter(1.46 * (1 + redshift), 1.27 * (1 + redshift), s=100, marker=\"x\")\n",
    "ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phases_on_coarse_freqs = np.array(\n",
    "    [\n",
    "        gw_search_functions.masses_to_phases(mm1, mm2, freqs_low_res)\n",
    "        for mm1, mm2 in zip(m1, m2)\n",
    "    ]\n",
    ")\n",
    "linear_free_phases = gw_search_functions.phases_to_linear_free_phases(\n",
    "    phases_on_coarse_freqs, freqs_low_res, amp_wht\n",
    ")\n",
    "\n",
    "phases_without_common_evolution = linear_free_phases - common_phase_evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=1)\n",
    "\n",
    "axs[0].plot(freqs_low_res, linear_free_phases[:64].T)\n",
    "axs[0].plot(freqs_low_res, common_phase_evolution, ls=\"--\", c=\"k\")\n",
    "axs[1].plot(freqs_low_res, phases_without_common_evolution[:64].T)\n",
    "for ax in axs:\n",
    "    ax.text(0.5, 0.95, r\"NOT REQUIRED\", color=\"red\", transform=ax.transAxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coordinates = (\n",
    "    svd_weights**2 * phases_without_common_evolution\n",
    ") @ phase_basis_coarse_freqs.T\n",
    "\n",
    "bank_coordinates, bank_indices = (\n",
    "    gw_search_functions.select_points_without_clutter(\n",
    "        coordinates, np.sqrt(0.1)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    *coordinates.T,\n",
    "    s=1,\n",
    "    alpha=0.5,\n",
    "    c=\"r\",\n",
    "    label=f\"full set ({len(coordinates)} points)\",\n",
    ")\n",
    "plt.scatter(\n",
    "    *bank_coordinates.T,\n",
    "    s=5,\n",
    "    c=\"k\",\n",
    "    label=f\"subset ({len(bank_coordinates)} points)\",\n",
    ")\n",
    "print(bank_coordinates.shape)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amp = np.zeros_like(freqs)\n",
    "amp[i1:] = freqs[i1:] ** (-7 / 6)\n",
    "normalization = gw_search_functions.correlate(amp, amp, w=whitening_filter)[\n",
    "    0\n",
    "] ** (1 / 2)\n",
    "amp /= normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# 10\n",
    "\n",
    "Repeat the search (sections 4-6, without repeating their plots) for each template in the bank individually (including glitch-removal). For each interval of 0.1 seconds, record which template gave the maximal SNR, and what was that SNR. **Plot the time-series of maximal $\\text{SNR}^2$ in per 0.1 seconds. Plot a histogram of the maximal values per 0.1 seconds**. *Before using the entire bank, try a small subset and see that the results make sense. The entire search could take several minutes, depending on hardware*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices_lists = []\n",
    "snr2_lists = []\n",
    "min_snr2_to_save = stats.chi2(df=2).isf(1 / (times[-1] / 0.1))\n",
    "glitch_test_threshold = stats.chi2(df=2).isf(0.01)\n",
    "snr2_lists_raw = []\n",
    "indices_lists_raw = []\n",
    "glitch_mask_list = []\n",
    "common_phase_evolution_high_res = np.interp(\n",
    "    x=freqs, xp=freqs_low_res, fp=common_phase_evolution\n",
    ")\n",
    "fs = 1 / dt\n",
    "t_start = time.time()\n",
    "\n",
    "for template_index, template_coordinate in tqdm(\n",
    "    enumerate(bank_coordinates), total=len(bank_coordinates), desc=\"template\"\n",
    "):\n",
    "    phase = common_phase_evolution_high_res + template_coordinate @ phase_basis\n",
    "    h = amp * np.exp(1j * phase)\n",
    "\n",
    "    snr2 = gw_search_functions.snr2_timeseries(\n",
    "        h * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "    h_low, h_high = np.zeros((2, len(freqs)), complex)\n",
    "    h_low[:i_fbar] = h[:i_fbar]\n",
    "    h_high[i_fbar:] = h[i_fbar:]\n",
    "    z_low = gw_search_functions.complex_overlap_timeseries(\n",
    "        h_low * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "    z_high = gw_search_functions.complex_overlap_timeseries(\n",
    "        h_high * whitening_filter, strain_f * whitening_filter\n",
    "    )\n",
    "\n",
    "    glitch_test_statistic = np.abs(z_low - z_high) ** 2\n",
    "\n",
    "    glitch_mask = (glitch_test_statistic > glitch_test_threshold) * (snr2 > 10)\n",
    "    glitch_mask_list.append(glitch_mask)\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2 * ~glitch_mask, int(0.1 * fs)\n",
    "    )\n",
    "    indices_lists.append(argmaxs)\n",
    "    snr2_lists.append(maxs)\n",
    "\n",
    "    maxs, argmaxs = gw_search_functions.max_argmax_over_n_samples(\n",
    "        snr2, int(0.1 * fs)\n",
    "    )\n",
    "    indices_lists_raw.append(argmaxs)\n",
    "    snr2_lists_raw.append(maxs)\n",
    "\n",
    "snr2_per_template = np.array(snr2_lists)\n",
    "time_indices_per_template = np.array(indices_lists)\n",
    "snr2_per_template_with_glitches = np.array(snr2_lists_raw)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bins = np.linspace(0, times[-1], snr2_per_template.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(time_bins, snr2_per_template.max(axis=0))\n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_ylabel(r\"Bestfit ${\\rm SNR}^2$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2_per_template.max(axis=0),\n",
    "    **hist_kwargs,\n",
    "    alpha=0.5,\n",
    "    label=\"With glitch removal\",\n",
    ")\n",
    "\n",
    "hist_kwargs = {\"histtype\": \"step\", \"density\": True, \"log\": True, \"bins\": 200}\n",
    "counts, edges, patches = ax.hist(\n",
    "    snr2_per_template_with_glitches.max(axis=0).flatten(),\n",
    "    label=\"Before glitch removal\",\n",
    "    **hist_kwargs,\n",
    "    ls=\"--\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(r\"${\\rm SNR}^2$\")\n",
    "ax.set_ylabel(\"counts (normalized)\")\n",
    "leg = ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 11.\n",
    "If you detected an event, **report its time, the masses of the template and an estimation or a upper bound of the false-alarm rate for such SNR**. Consider the number of templates you used and the fact that waveforms have typical auto-correlation length of 1 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with https://arxiv.org/pdf/1710.05832 Table 1\n",
    "best_template_index, best_timestamp_index = np.unravel_index(\n",
    "    snr2_per_template.argmax(), snr2_per_template.shape\n",
    ")\n",
    "bestfit_m1 = m1[best_template_index]\n",
    "bestfit_m2 = m2[best_template_index]\n",
    "bestfit_mchirp = gw_search_functions.m1m2_to_mchirp(bestfit_m1, bestfit_m2)\n",
    "bestfit_snr2 = snr2_per_template.max()\n",
    "time_bins = np.linspace(0, times[-1], snr2_per_template.shape[1])\n",
    "bestfit_time = snr2_per_template.max(axis=0).argmax()\n",
    "\n",
    "print(\"NOT REQUIRED\")\n",
    "print(f\"Maximal SNR^2 found : {bestfit_snr2:.5g} at time {bestfit_time:.3g}\")\n",
    "print(\n",
    "    f\"Template of masses ({bestfit_m1:.3g},{bestfit_m2:.3g}), or chirp-mass {bestfit_mchirp:.5g} (solar masses)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "\n",
    "p = Probability that non of N = N_templates * N_times individual experiments will reach value x or higher :\n",
    "$$p = 1 - (SF(x))^N$$\n",
    "\n",
    "Probability that at least one of $N$ experiments will reach value x:\n",
    "$1 - p = (SF(x))^N $\n",
    "\n",
    "At this high SNR^2, the FAR is almost exactly zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_templates = bank_coordinates.shape[0]\n",
    "N_trials = N_templates * len(times)\n",
    "\n",
    "stats.chi2(df=2).sf(snr2_per_template.max()) ** N_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.log(1 - stats.chi2(df).sf(snr2_per_template.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "tags": []
   },
   "source": [
    "use binomial to imporve the calculation\n",
    "\n",
    "$$ FAR = 1 - CDF(x)^N = 1 - (1-SF(x))^N \\approx 1 - 1 + N\\cdot SF(x) = N\\cdot SF(x) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "snr2_per_template.size * stats.chi2(df=2).sf(snr2_per_template.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## 12.\n",
    "\n",
    "**Create a spectogram (using e.g. `matplotlib.pyplot.specgram`), localized in time and frequency around the event you found**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the histogram in 2 steps. So I can calibrate the dynamic range in the second histogram using the fist histogram\n",
    "specgram_kwargs = {\n",
    "    \"x\": np.fft.irfft(strain_f * whitening_filter),\n",
    "    \"NFFT\": int(fs * 0.5),\n",
    "    \"noverlap\": int(fs * 0.25),\n",
    "    \"scale\": \"linear\",\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 25,\n",
    "    \"Fs\": fs,\n",
    "}\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "specgram_kwargs = {\n",
    "    \"x\": np.fft.irfft(strain_f * whitening_filter),\n",
    "    \"NFFT\": int(fs * 0.5),\n",
    "    \"noverlap\": int(fs * 0.25),\n",
    "    \"scale\": \"linear\",\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": o[0][(o[1] > 20) * (o[1] < 1000)].std() * 5,\n",
    "    \"Fs\": fs,\n",
    "}\n",
    "\n",
    "o = plt.specgram(**specgram_kwargs)\n",
    "tmin = 0.1 * 10259 - 6\n",
    "tmax = 0.1 * 10259 + 1\n",
    "fmin = 20\n",
    "fmax = 1000\n",
    "plt.xlim(tmin, tmax)\n",
    "plt.ylim(20, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME1 = time.time()\n",
    "\n",
    "print(f\"Time passed: {TIME1 - TIME0:.3g} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gw_detection_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
